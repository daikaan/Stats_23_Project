---
title: "sl_project"
output:
  html_document: default
  pdf_document: default
date: "2023-06-01"
---

Import Libraries We Need
```{r}
library(dplyr)
library(corrplot)
library(caTools)
library(ggpubr)
library(ROSE)
library(correlation)
library(moments) 
library(olsrr) 
library(MASS)
library(knitr)
library(forecast)
library(ggplot2)
library(PCAmixdata)
library(purrr)
```

Introduction to Data

In this project we are going to predict the probability of a customer attrition. We have a data contains demographic information about customers and their spending behavior. We downloaded the dataset from the Kaggle. Here is the link for the dataset. It can be found publicly.
https://www.kaggle.com/datasets/thedevastator/predicting-credit-card-customer-attrition-with-m

```{r}
bank_data_origin <- read.csv('~/GitHub/Stats_23_Project/BankChurners.csv')
head(bank_data_origin)
```

In the beginning we have 23 columns and 10127 rows, but we have 2 columsn named as "Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1" and "Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2". We don't need these 2 columns and deleting them will not affect any other columns. Finally, we have 21 columns and 10127 rows in the beginning.

```{r}
dim(bank_data_origin)
bank_data_origin <- subset(bank_data_origin, select = -c(Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1, Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2))
final_dim <- dim(bank_data_origin)
final_dim

```

Our data has 15 numerical and 6 categorical columns.

```{r}
map(bank_data_origin,class)
```

Data Exploration

There is no null value inside our data 

```{r}
colSums(is.na(bank_data_origin))
```

Education Level, Marital Status and Income Category columns have "Unknown" values. Since unknown values can cause misleading results, we decide to remove them. First we labeled them with "NA" and delete afterwards.

```{r}
bank_data_NA <- bank_data_origin
bank_data_NA[bank_data_NA=='Unknown'] <- NA

colSums(is.na(bank_data_NA))

bank_data_withoutNA <- na.omit(bank_data_NA)
```

For the 6 categorical columns, we change their data type to the numeric to be able to work with them.

```{r}
bank_data_quan <- bank_data_withoutNA

bank_data_quan$Attrition_Flag <- as.numeric(bank_data_quan$Attrition_Flag == "Attrited Customer")

bank_data_quan$Gender <- as.numeric(bank_data_quan$Gender == "F")
bank_data_quan <- bank_data_quan %>% rename("Is_Female" = "Gender")

order_education_level <- list("Uneducated" = 1,
                              "High School" = 2,
                              "College" = 3,
                              "Graduate" = 4,
                              "Post-Graduate" = 5,
                              "Doctorate" = 6)
bank_data_quan$Education_Level <- unlist(order_education_level[as.character(bank_data_quan$Education_Level)])

order_Marital_Status <- list("Single" = 1,
                             "Married" = 2,
                             "Divorced" = 3)
bank_data_quan$Marital_Status <- unlist(order_Marital_Status[as.character(bank_data_quan$Marital_Status)])

order_Income_Category <- list("Less than $40K" = 1,
                              "$40K - $60K" = 2,
                              "$60K - $80K" = 3,
                              "$80K - $120K" = 4,
                              "$120K +" = 5)
bank_data_quan$Income_Category <- unlist(order_Income_Category[as.character(bank_data_quan$Income_Category)])


order_Card_Category <- list("Blue" = 1,
                            "Silver" = 2,
                            "Gold" = 3,
                            "Platinum" = 4)
bank_data_quan$Card_Category <- unlist(order_Card_Category[as.character(bank_data_quan$Card_Category)])

colSums(is.na(bank_data_quan))

```

Finally, since the people with age greater than or equal to 70 years and Card Category different than the blue is outliers, we remove them to our dataset.

```{r}
cleaned_bank_data <- bank_data_quan

age.exc.list <- boxplot.stats(cleaned_bank_data$Customer_Age)$out
card.exc.list <- c(2, 3, 4)

cleaned_bank_data <- subset(cleaned_bank_data,!((Customer_Age %in% age.exc.list)| (Card_Category %in% card.exc.list)))
dim(cleaned_bank_data)
```

Data Preparation

Since we have 0 and 1's as outcome and the outcomes for each row are independent we can use Logistic Regression. In our data, we are going to predict a probability of credit card churn. As a result, we will get a value between 0 and 1. 

Since we have only one card category, we can remove Card Category column from now on.

```{r}
cleaned_bank_data <- subset(cleaned_bank_data, select = -c(Card_Category))
head(cleaned_bank_data)
```
We are going to create our Logistic Regression model with 75% train size.
```{r}
set.seed(0987)

sample <- sample.split(cleaned_bank_data$Attrition_Flag,SplitRatio = 0.75)
train <- subset(cleaned_bank_data[2:20],sample == TRUE)
test <- subset(cleaned_bank_data[2:20],sample == FALSE)

train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(cleaned_bank_data))$data

model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```


```{r}
cor_mat_new <- cor(cleaned_bank_data[2:20])
corrplot(cor_mat_new,method = "number",type = "upper", tl.pos = "td",tl.cex=0.5, tl.col = "black" ,diag = FALSE)
```
Since we have almost perfectly correlation between Credit Limit and Average Open to Buy columns, we got NA on the summary. Because they will not given independent information in the model. So, we can drop Average Open to Buy

```{r}
set.seed(0987)

cleaned_bank_data1 <- subset(cleaned_bank_data, select = -c(Avg_Open_To_Buy))

sample <- sample.split(cleaned_bank_data1$Attrition_Flag,SplitRatio = 0.75)
train <- subset(cleaned_bank_data1[2:19],sample == TRUE)
test <- subset(cleaned_bank_data1[2:19],sample == FALSE)

train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(cleaned_bank_data1))$data

model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```
From now on, we assume 95% confidence interwal for our model. We will delete columns with high p values by one by until we get all columns with at least '*'.

So as a first step, we delete Months On Book.


```{r}
set.seed(0987)

cleaned_bank_data2 <- subset(cleaned_bank_data1, select = -c(Months_on_book))

sample <- sample.split(cleaned_bank_data2$Attrition_Flag,SplitRatio = 0.75)
train <- subset(cleaned_bank_data2[2:18],sample == TRUE)
test <- subset(cleaned_bank_data2[2:18],sample == FALSE)

train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(cleaned_bank_data2))$data

model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```
Second step, delete Total Amount Change Q4-1.

```{r}
set.seed(0987)

cleaned_bank_data3 <- subset(cleaned_bank_data2, select = -c(Total_Amt_Chng_Q4_Q1))

sample <- sample.split(cleaned_bank_data3$Attrition_Flag,SplitRatio = 0.75)
train <- subset(cleaned_bank_data3[2:17],sample == TRUE)
test <- subset(cleaned_bank_data3[2:17],sample == FALSE)

train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(cleaned_bank_data3))$data

model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```
Third step, delete Education Level.

```{r}
set.seed(0987)

cleaned_bank_data4 <- subset(cleaned_bank_data3, select = -c(Education_Level))

sample <- sample.split(cleaned_bank_data4$Attrition_Flag,SplitRatio = 0.75)
train <- subset(cleaned_bank_data4[2:16],sample == TRUE)
test <- subset(cleaned_bank_data4[2:16],sample == FALSE)

train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(cleaned_bank_data4))$data

model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```
Finally, we get 88.7% Credit Card Churn accuracy, but we can increase the accuracy with normalizing all the available columns. If we normalize all the columns, model converges faster.

In order to make the columns, we investigate the skewness of the each column. If the skewness between -1 and 1, we can say the column is normally distributed. Otherwise, we should rescale the column with applying some functions and look their skewness again until we get desired value.

```{r}
hist(cleaned_bank_data$Customer_Age, main = "Histogram for Customer Age", xlab = "Customer_Age", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Customer_Age)

hist(cleaned_bank_data$Is_Female, main = "Histogram for Sex", xlab = "Is_Female", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Is_Female)

hist(cleaned_bank_data$Dependent_count, main = "Histogram for Dependent Count", xlab = "Dependent_count", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Dependent_count)

hist(cleaned_bank_data$Education_Level, main = "Histogram for Education Level", xlab = "Education Level", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Education_Level)

hist(cleaned_bank_data$Marital_Status, main = "Histogram for Marital Status", xlab = "Marital_Status", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Marital_Status)

hist(cleaned_bank_data$Income_Category, main = "Histogram for Income Category", xlab = "Income_Category", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Income_Category)

hist(cleaned_bank_data$Months_on_book, main = "Histogram for Months On Book", xlab = "Months_on_book", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Months_on_book)

hist(cleaned_bank_data$Total_Relationship_Count, main = "Histogram for Total Relationship Count", xlab = "Total_Relationship_Count", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Total_Relationship_Count)

hist(cleaned_bank_data$Months_Inactive_12_mon, main = "Histogram for Months Inactive in 12 Months", xlab = "Months_Inactive_12_mon", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Months_Inactive_12_mon)

hist(cleaned_bank_data$Contacts_Count_12_mon, main = "Histogram for Contacts Count in 12 Months", xlab = "Contacts_Count_12_mon", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Contacts_Count_12_mon)

hist(cleaned_bank_data$Credit_Limit, main = "Histogram for Credit Limit", xlab = "Credit_Limit", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Credit_Limit)

hist(log1p(cleaned_bank_data$Credit_Limit), main = "Histogram for Log Credit Limit", xlab = "Log_Credit_Limit", ylab = "Count", col = "cornflowerblue")

skewness(log1p(cleaned_bank_data$Credit_Limit))

hist(cleaned_bank_data$Total_Revolving_Bal, main = "Histogram for Total Revolving Balance", xlab = "Total_Revolving_Bal", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Total_Revolving_Bal)

hist(cleaned_bank_data$Avg_Open_To_Buy, main = "Histogram for Average Open To Buy", xlab = "Avg_Open_To_Buy", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Avg_Open_To_Buy)

hist(log1p(cleaned_bank_data$Avg_Open_To_Buy), main = "Histogram for Log Average Open To Buy", xlab = "Log_Avg_Open_To_Buy", ylab = "Count", col = "cornflowerblue")

skewness(log1p(cleaned_bank_data$Avg_Open_To_Buy))

hist(cleaned_bank_data$Total_Amt_Chng_Q4_Q1, main = "Histogram for Total Amount Change Q4-Q1", xlab = "Total_Amt_Chng_Q4_Q1", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Total_Amt_Chng_Q4_Q1)

hist(log1p(cleaned_bank_data$Total_Amt_Chng_Q4_Q1), main = "Histogram for Log Total Amount Change Q4-Q1", xlab = "Log_Total_Amt_Chng_Q4_Q1", ylab = "Count", col = "cornflowerblue")

skewness(log1p(cleaned_bank_data$Total_Amt_Chng_Q4_Q1))

hist(cleaned_bank_data$Total_Trans_Amt, main = "Histogram for Total Transaction Amount", xlab = "Total_Trans_Amt", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Total_Trans_Amt)

hist(log1p(cleaned_bank_data$Total_Trans_Amt), main = "Histogram for Log Total Transaction Amount", xlab = "Log_Total_Trans_Amt", ylab = "Count", col = "cornflowerblue")

skewness(log1p(cleaned_bank_data$Total_Trans_Amt))

hist(cleaned_bank_data$Total_Trans_Ct, main = "Histogram for Total Transaction Count", xlab = "Total_Trans_Ct", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Total_Trans_Ct)

hist(cleaned_bank_data$Total_Ct_Chng_Q4_Q1, main = "Histogram for Total Count Change in 12 Months", xlab = "Total_Ct_Chng_Q4_Q1", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Total_Ct_Chng_Q4_Q1)

hist(log1p(cleaned_bank_data$Total_Ct_Chng_Q4_Q1), main = "Histogram for Log Total Count Change in 12 Months", xlab = "Log_Total_Ct_Chng_Q4_Q1", ylab = "Count", col = "cornflowerblue")

skewness(log1p(cleaned_bank_data$Total_Ct_Chng_Q4_Q1))

hist(cleaned_bank_data$Avg_Utilization_Ratio, main = "Histogram for Average Utilitization Ratio", xlab = "Avg_Utilization_Ratio", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Avg_Utilization_Ratio)
```

Create new table with taking logarithm for the columns:
Credit_Limit
Avg_Open_To_Buy
Total_Amt_Chng_Q4_Q1
Total_Trans_Amt
Total_Ct_Chng_Q4_Q1

```{r}
log_cleaned_bank_data <- cleaned_bank_data

log_cleaned_bank_data$Credit_Limit <- log1p(log_cleaned_bank_data$Credit_Limit)
colnames(log_cleaned_bank_data)[13] <- "log_Credit_Limit"

log_cleaned_bank_data$Avg_Open_To_Buy <- log1p(log_cleaned_bank_data$Avg_Open_To_Buy)
colnames(log_cleaned_bank_data)[15] <- "log_Avg_Open_To_Buy"

log_cleaned_bank_data$Total_Amt_Chng_Q4_Q1 <- log1p(log_cleaned_bank_data$Total_Amt_Chng_Q4_Q1)
colnames(log_cleaned_bank_data)[16] <- "log_Total_Amt_Chng_Q4_Q1"

log_cleaned_bank_data$Total_Trans_Amt <- log1p(log_cleaned_bank_data$Total_Trans_Amt)
colnames(log_cleaned_bank_data)[17] <- "log_Total_Trans_Amt"

log_cleaned_bank_data$Total_Ct_Chng_Q4_Q1 <- log1p(log_cleaned_bank_data$Total_Ct_Chng_Q4_Q1)
colnames(log_cleaned_bank_data)[19] <- "log_Total_Ct_Chng_Q4_Q1"

head(log_cleaned_bank_data)
```

And now, create Logistic Regression Model again with the latest table.

```{r}
set.seed(0987)

sample <- sample.split(log_cleaned_bank_data$Attrition_Flag,SplitRatio = 0.75)
train <- subset(log_cleaned_bank_data[2:20],sample == TRUE)
test <- subset(log_cleaned_bank_data[2:20],sample == FALSE)

train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(log_cleaned_bank_data))$data

model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```

First, we should remove Months On Book.

```{r}
log_cleaned_bank_data1 <- subset(log_cleaned_bank_data, select = -c(Months_on_book))

set.seed(0987)

sample <- sample.split(log_cleaned_bank_data1$Attrition_Flag,SplitRatio = 0.75)
train <- subset(log_cleaned_bank_data1[2:19],sample == TRUE)
test <- subset(log_cleaned_bank_data1[2:19],sample == FALSE)

# Under-sampling
train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

# Over-sampling
train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

#Mixed Sampling with 40% of Attrited Customer

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(log_cleaned_bank_data1))$data


model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```
Than, we should remove Education Level.

```{r}
log_cleaned_bank_data2 <- subset(log_cleaned_bank_data1, select = -c(Education_Level))

sample <- sample.split(log_cleaned_bank_data2$Attrition_Flag,SplitRatio = 0.75)
train <- subset(log_cleaned_bank_data2[2:18],sample == TRUE)
test <- subset(log_cleaned_bank_data2[2:18],sample == FALSE)

# Under-sampling
train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

# Over-sampling
train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

#Mixed Sampling with 40% of Attrited Customer

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(log_cleaned_bank_data2))$data


model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```
Finally, we should remove Dependent Count.

```{r}
log_cleaned_bank_data3 <- subset(log_cleaned_bank_data2, select = -c(Dependent_count))

sample <- sample.split(log_cleaned_bank_data3$Attrition_Flag,SplitRatio = 0.75)
train <- subset(log_cleaned_bank_data3[2:17],sample == TRUE)
test <- subset(log_cleaned_bank_data3[2:17],sample == FALSE)

# Under-sampling
train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

# Over-sampling
train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

#Mixed Sampling with 40% of Attrited Customer

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(log_cleaned_bank_data3))$data


model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```

As a result, we get 90.4% Credit Card Churn accuracy.
