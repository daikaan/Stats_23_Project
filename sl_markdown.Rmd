---
title: "sl_project"
output: html_document
date: "2023-06-01"
---

Import Libraries We Need
```{r}
library(dplyr)
library(corrplot)
library(caTools)
library(ggpubr)
library(ROSE)
library(correlation)
library(moments) 
library(olsrr) 
library(MASS)
library(knitr)
library(forecast)
library(ggplot2)
library(PCAmixdata)
library(purrr)
```

Introduction to Data

In this project we are going to predict the probability of a customer attrition. We have a data contains demographic information about customers and their spending behavior. We downloaded the dataset from the Kaggle. Here is the link for the dataset. It can be found publicly.
https://www.kaggle.com/datasets/thedevastator/predicting-credit-card-customer-attrition-with-m

```{r}
bank_data_origin <- read.csv('~/GitHub/Stats_23_Project/BankChurners.csv')
head(bank_data_origin)
```

In the beginning we have 23 columns and 10127 rows, but we have 2 columsn named as "Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1" and "Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2". We don't need these 2 columns and deleting them will not affect any other columns. Finally, we have 21 columns and 10127 rows in the beginning.

```{r}
dim(bank_data_origin)
bank_data_origin <- subset(bank_data_origin, select = -c(Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1, Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2))
final_dim <- dim(bank_data_origin)
final_dim

```

Our data has 15 numerical and 6 categorical columns.

```{r}
map(bank_data_origin,class)
```

Data Exploration

There is no null value inside our data 

```{r}
colSums(is.na(bank_data_origin))
```

Education Level, Marital Status and Income Category columns have "Unknown" values. Since unknown values can cause misleading results, we decide to remove them. First we labeled them with "NA" and delete afterwards.

```{r}
bank_data_NA <- bank_data_origin
bank_data_NA[bank_data_NA=='Unknown'] <- NA

colSums(is.na(bank_data_NA))

bank_data_withoutNA <- na.omit(bank_data_NA)
```

For the 6 categorical columns, we change their data type to the numeric to be able to work with them.

```{r}
bank_data_quan <- bank_data_withoutNA

bank_data_quan$Attrition_Flag <- as.numeric(bank_data_quan$Attrition_Flag == "Attrited Customer")

bank_data_quan$Gender <- as.numeric(bank_data_quan$Gender == "F")
bank_data_quan <- bank_data_quan %>% rename("Is_Female" = "Gender")

order_education_level <- list("Uneducated" = 1,
                              "High School" = 2,
                              "College" = 3,
                              "Graduate" = 4,
                              "Post-Graduate" = 5,
                              "Doctorate" = 6)
bank_data_quan$Education_Level <- unlist(order_education_level[as.character(bank_data_quan$Education_Level)])

order_Marital_Status <- list("Single" = 1,
                             "Married" = 2,
                             "Divorced" = 3)
bank_data_quan$Marital_Status <- unlist(order_Marital_Status[as.character(bank_data_quan$Marital_Status)])

order_Income_Category <- list("Less than $40K" = 1,
                              "$40K - $60K" = 2,
                              "$60K - $80K" = 3,
                              "$80K - $120K" = 4,
                              "$120K +" = 5)
bank_data_quan$Income_Category <- unlist(order_Income_Category[as.character(bank_data_quan$Income_Category)])


order_Card_Category <- list("Blue" = 1,
                            "Silver" = 2,
                            "Gold" = 3,
                            "Platinum" = 4)
bank_data_quan$Card_Category <- unlist(order_Card_Category[as.character(bank_data_quan$Card_Category)])

colSums(is.na(bank_data_quan))

```

Finally, since the people with age greater than or equal to 70 years and Card Category different than the blue is outliers, we remove them to our dataset.

```{r}
cleaned_bank_data <- bank_data_quan

age.exc.list <- boxplot.stats(cleaned_bank_data$Customer_Age)$out
card.exc.list <- c(2, 3, 4)

cleaned_bank_data <- subset(cleaned_bank_data,!((Customer_Age %in% age.exc.list)| (Card_Category %in% card.exc.list)))
dim(cleaned_bank_data)
```

Data Preparation

Since we have 0 and 1's as outcome and the outcomes for each row are independent we can use Logistic Regression. In our data, we are going to predict a probability of credit card churn. As a result, we will get a value between 0 and 1. 

Since we have only one card category, we can remove Card Category column from now on.

```{r}
cleaned_bank_data <- subset(cleaned_bank_data, select = -c(Card_Category))
head(cleaned_bank_data)
```
We are going to create our Logistic Regression model with 75% train size.
```{r}
set.seed(0987)

sample <- sample.split(cleaned_bank_data$Attrition_Flag,SplitRatio = 0.75)
train <- subset(cleaned_bank_data[2:20],sample == TRUE)
test <- subset(cleaned_bank_data[2:20],sample == FALSE)

train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(cleaned_bank_data))$data

model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```


```{r}
cor_mat_new <- cor(cleaned_bank_data[2:20])
corrplot(cor_mat_new,method = "number",type = "upper", tl.pos = "td",tl.cex=0.5, tl.col = "black" ,diag = FALSE)
```
Since we have almost perfectly correlation between Credit Limit and Average Open to Buy columns, we got NA on the summary. Because they will not given independent information in the model. So, we can drop Average Open to Buy

```{r}
set.seed(0987)

cleaned_bank_data1 <- subset(cleaned_bank_data, select = -c(Avg_Open_To_Buy))

sample <- sample.split(cleaned_bank_data1$Attrition_Flag,SplitRatio = 0.75)
train <- subset(cleaned_bank_data1[2:19],sample == TRUE)
test <- subset(cleaned_bank_data1[2:19],sample == FALSE)

train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(cleaned_bank_data1))$data

model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```
From now on, we assume 95% confidence interwal for our model. We will delete columns with high p values by one by until we get all columns with at least '*'.

So as a first step, we delete Months On Book.


```{r}
set.seed(0987)

cleaned_bank_data2 <- subset(cleaned_bank_data1, select = -c(Months_on_book))

sample <- sample.split(cleaned_bank_data2$Attrition_Flag,SplitRatio = 0.75)
train <- subset(cleaned_bank_data2[2:18],sample == TRUE)
test <- subset(cleaned_bank_data2[2:18],sample == FALSE)

train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(cleaned_bank_data2))$data

model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```
Second step, delete Total Amount Change Q4-1.

```{r}
set.seed(0987)

cleaned_bank_data3 <- subset(cleaned_bank_data2, select = -c(Total_Amt_Chng_Q4_Q1))

sample <- sample.split(cleaned_bank_data3$Attrition_Flag,SplitRatio = 0.75)
train <- subset(cleaned_bank_data3[2:17],sample == TRUE)
test <- subset(cleaned_bank_data3[2:17],sample == FALSE)

train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(cleaned_bank_data3))$data

model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```
Third step, delete Education Level.

```{r}
set.seed(0987)

cleaned_bank_data4 <- subset(cleaned_bank_data3, select = -c(Education_Level))

sample <- sample.split(cleaned_bank_data4$Attrition_Flag,SplitRatio = 0.75)
train <- subset(cleaned_bank_data4[2:16],sample == TRUE)
test <- subset(cleaned_bank_data4[2:16],sample == FALSE)

train_under <- ovun.sample(Attrition_Flag~.,data = train, method = "under")$data

train_over <- ovun.sample(Attrition_Flag~.,data = train, method = "over")$data

train_mix <- ovun.sample(Attrition_Flag~.,data = train, method = "both", p = 0.4, N = nrow(cleaned_bank_data4))$data

model <- glm(Attrition_Flag ~ ., data = train_mix, family = 'binomial')
summary(model)$coeff

summary(model)

pred <- (predict(model, test) >= 0.5)*1

mean(test$Attrition_Flag == pred)
```
Finally, we get 88.7% Credit Card Churn accuracy, but we can increase the accuracy with normalizing all the available columns. If we normalize all the columns, model converges faster.

```{r}
hist(cleaned_bank_data$Customer_Age, main = "Histogram for Customer Age", xlab = "Customer_Age", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Customer_Age)

hist(cleaned_bank_data$Dependent_count, main = "Histogram for Dependent Count", xlab = "Dependent_count", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Dependent_count)

hist(cleaned_bank_data$Months_on_book, main = "Histogram for Months On Book", xlab = "Months_on_book", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Months_on_book)

hist(cleaned_bank_data$Total_Relationship_Count, main = "Histogram for Total Relationship Count", xlab = "Total_Relationship_Count", ylab = "Count", col = "cornflowerblue")

skewness(cleaned_bank_data$Total_Relationship_Count)

```
