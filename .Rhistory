+ log_Total_Amt_Chng_Q4_Q1:Total_Trans_Ct + log_Total_Amt_Chng_Q4_Q1:log_Total_Trans_Amt + log_Credit_Limit:Total_Revolving_Bal
+ Dependent_count:log_Total_Amt_Chng_Q4_Q1 + Total_Revolving_Bal:Avg_Utilization_Ratio, data = train, family = "binomial")
qda_1
qda_1_predict <- predict(qda_1,test,type = "response")
qda_predict_1 <- qda_1_predict$posterior
pred_1 <- ifelse(qda_predict_1[,2] >= Threshold1 , 1,0)
pred_2 <- ifelse(qda_predict_1[,2] >= Threshold2 , 1,0)
pred_3 <- ifelse(qda_predict_1[,2] >= Threshold3 , 1,0)
#Confusion matrix
c_mat_1 <- table(test$Attrition_Flag,pred_1)
c_mat_2 <- table(test$Attrition_Flag,pred_2)
c_mat_3 <- table(test$Attrition_Flag,pred_3)
c_mat_1
c_mat_2
c_mat_3
#Accuracy
mean(pred_1==test$Attrition_Flag)*100
mean(pred_2==test$Attrition_Flag)*100
mean(pred_3==test$Attrition_Flag)*100
#True Negative Rate / Specificity
Spec_1 <- c_mat_1[1,1]/sum(c_mat_1[1,])
Spec_2 <- c_mat_2[1,1]/sum(c_mat_2[1,])
Spec_3 <- c_mat_3[1,1]/sum(c_mat_3[1,])
Spec_1
Spec_2
Spec_3
#Precision / Positive Predicted Value
Prec_1 <- c_mat_1[2,2]/sum(c_mat_1[,2])
Prec_2 <- c_mat_2[2,2]/sum(c_mat_2[,2])
Prec_3 <- c_mat_3[2,2]/sum(c_mat_3[,2])
Prec_1
Prec_2
Prec_3
#Recall / True Positive Rate / Sensitivity
Rec_1 <- c_mat_1[2,2]/sum(c_mat_1[2,])
Rec_2 <- c_mat_2[2,2]/sum(c_mat_2[2,])
Rec_3 <- c_mat_3[2,2]/sum(c_mat_3[2,])
Rec_1
Rec_2
Rec_3
#F1 Score
F1_1 <- 2 * (Prec_1 * Rec_1)/(Prec_1 + Rec_1)
F1_2 <- 2 * (Prec_2 * Rec_2)/(Prec_2 + Rec_2)
F1_3 <- 2 * (Prec_3 * Rec_3)/(Prec_3 + Rec_3)
F1_1
F1_2
F1_3
#QDA Balanced
#Thresholds
Threshold1 <- 0.7
Threshold2 <- 0.8
Threshold3 <- 0.9
#We still try the QDA model
qda_2 <- qda(Attrition_Flag ~  Customer_Age + Is_Female + Dependent_count + Education_Level
+ Marital_Status + Income_Category + Total_Relationship_Count + Months_Inactive_12_mon + Contacts_Count_12_mon + Total_Revolving_Bal + log_Avg_Open_To_Buy
+ log_Total_Amt_Chng_Q4_Q1 + log_Total_Trans_Amt + Total_Trans_Ct + log_Total_Ct_Chng_Q4_Q1 + Avg_Utilization_Ratio + log_Total_Amt_Chng_Q4_Q1:log_Total_Trans_Amt
+ Total_Revolving_Bal:log_Avg_Open_To_Buy + Dependent_count:log_Total_Amt_Chng_Q4_Q1 + Total_Revolving_Bal:Avg_Utilization_Ratio, data = train_bal, family = "binomial")
qda_2
qda_2_predict <- predict(qda_2,test,type = "response")
qda_predict_2 <- qda_2_predict$posterior
pred_1 <- ifelse(qda_predict_2[,2] >= Threshold1 , 1,0)
pred_2 <- ifelse(qda_predict_2[,2] >= Threshold2 , 1,0)
pred_3 <- ifelse(qda_predict_2[,2] >= Threshold3 , 1,0)
#Confusion matrix
c_mat_1 <- table(test$Attrition_Flag,pred_1)
c_mat_2 <- table(test$Attrition_Flag,pred_2)
c_mat_3 <- table(test$Attrition_Flag,pred_3)
c_mat_1
c_mat_2
c_mat_3
#Accuracy
mean(pred_1==test$Attrition_Flag)*100
mean(pred_2==test$Attrition_Flag)*100
mean(pred_3==test$Attrition_Flag)*100
#True Negative Rate / Specificity
Spec_1 <- c_mat_1[1,1]/sum(c_mat_1[1,])
Spec_2 <- c_mat_2[1,1]/sum(c_mat_2[1,])
Spec_3 <- c_mat_3[1,1]/sum(c_mat_3[1,])
Spec_1
Spec_2
Spec_3
#Precision / Positive Predicted Value
Prec_1 <- c_mat_1[2,2]/sum(c_mat_1[,2])
Prec_2 <- c_mat_2[2,2]/sum(c_mat_2[,2])
Prec_3 <- c_mat_3[2,2]/sum(c_mat_3[,2])
Prec_1
Prec_2
Prec_3
#Recall / True Positive Rate / Sensitivity
Rec_1 <- c_mat_1[2,2]/sum(c_mat_1[2,])
Rec_2 <- c_mat_2[2,2]/sum(c_mat_2[2,])
Rec_3 <- c_mat_3[2,2]/sum(c_mat_3[2,])
Rec_1
Rec_2
Rec_3
#F1 Score
F1_1 <- 2 * (Prec_1 * Rec_1)/(Prec_1 + Rec_1)
F1_2 <- 2 * (Prec_2 * Rec_2)/(Prec_2 + Rec_2)
F1_3 <- 2 * (Prec_3 * Rec_3)/(Prec_3 + Rec_3)
F1_1
F1_2
F1_3
#ROC curves
roc_qda <- roc(test$Attrition_Flag ~ qda_predict_1[,2])
roc_qda_bal <- roc(test$Attrition_Flag ~ qda_predict_2[,2])
AUC_qda <- auc(roc_qda)
AUC_qda_bal <- auc(roc_qda_bal)
plot(roc_qda, col = "black",print.auc = TRUE, auc.polygon = TRUE, max.auc.polygon = TRUE, lwd=2,print.auc.x = 0.5,print.auc.y = 0.5)
plot(roc_qda_bal,add = TRUE,col = "blue", print.auc = TRUE, lwd=2, print.auc.x = 0.5,print.auc.y = 0.43)
#Best thresholds and Best Sensitivity and Specificity
Best_Treshold_qda <- coords(roc_qda,"best",best.method = "closest.topleft")$threshold
Best_pred_qda <- ifelse(qda_predict_1[,2] >= Best_Treshold_qda , 1,0)
Best_c_mat <- table(test$Attrition_Flag,Best_pred_qda)
Best_Spec_qda <- Best_c_mat[1,1]/sum(Best_c_mat[1,])
Best_Sens_qda <- Best_c_mat[2,2]/sum(Best_c_mat[2,])
Best_Treshold_qda_bal <- coords(roc_qda_bal,"best",best.method = "closest.topleft")$threshold
Best_pred_qda_bal <- ifelse(qda_predict_2[,2] >= Best_Treshold_qda_bal , 1,0)
Best_c_mat <- table(test$Attrition_Flag,Best_pred_qda_bal)
Best_Spec_qda_bal <- Best_c_mat[1,1]/sum(Best_c_mat[1,])
Best_Sens_qda_bal <- Best_c_mat[2,2]/sum(Best_c_mat[2,])
#Table to showing them
Table_mat <-  matrix(c(Best_Treshold_qda,Best_Spec_qda,Best_Sens_qda,Best_Treshold_qda_bal,Best_Spec_qda_bal,Best_Sens_qda_bal), ncol=3, byrow=TRUE)
colnames(Table_mat) <- c("Threshold","Specificity","Sensitivity")
rownames(Table_mat) <- c("Unbalanced","Balanced")
Tab <- as.table(Table_mat)
show(Tab)
#Ridge
#Creation of train and test set with interactions
train_int <- data.frame(train)
train_int$log_Total_Amt_Chng_Q4_Q1_Total_Trans_Ct <- train$log_Total_Amt_Chng_Q4_Q1*train$Total_Trans_Ct
train_int$log_Total_Amt_Chng_Q4_Q1_log_Total_Trans_Amt <- train$log_Total_Amt_Chng_Q4_Q1*train$log_Total_Trans_Amt
train_int$log_Credit_Limit_Total_Revolving_Bal <- train$log_Credit_Limit*train$Total_Revolving_Bal
train_int$log_Total_Amt_Chng_Q4_Q1_Dependent_count <- train$log_Total_Amt_Chng_Q4_Q1*train$Dependent_count
train_int$Total_Revolving_Bal_Avg_Utilization_Ratio <- train$Total_Revolving_Bal*train$Avg_Utilization_Ratio
test_int <- data.frame(test)
test_int$log_Total_Amt_Chng_Q4_Q1_Total_Trans_Ct <- test$log_Total_Amt_Chng_Q4_Q1*test$Total_Trans_Ct
test_int$log_Total_Amt_Chng_Q4_Q1_log_Total_Trans_Amt <- test$log_Total_Amt_Chng_Q4_Q1*test$log_Total_Trans_Amt
test_int$log_Credit_Limit_Total_Revolving_Bal <- test$log_Credit_Limit*test$Total_Revolving_Bal
test_int$log_Total_Amt_Chng_Q4_Q1_Dependent_count <- test$log_Total_Amt_Chng_Q4_Q1*test$Dependent_count
test_int$Total_Revolving_Bal_Avg_Utilization_Ratio <- test$Total_Revolving_Bal*test$Avg_Utilization_Ratio
#We transorm them into matrix withou Attrition_Flag, Education_Level , Months_on_book and log_Avg_Open_To_Buy
train_mat <- data.matrix(train_int[,-c(1,5,8,14)])
test_mat <- data.matrix(test_int[,-c(1,5,8,14)])
ridge <- cv.glmnet(train_mat,train$Attrition_Flag, alpha = 0, family = "binomial", type.measure = "class")
plot(ridge)
opt_lambda_ridge <- ridge$lambda.min
opt_lambda_ridge
ridge_predict <- predict(ridge,test_mat,type = "class", s = opt_lambda_ridge)
#Confusion matrix
c_mat_ridge <- table(test$Attrition_Flag,ridge_predict)
c_mat_ridge
#Accuracy
mean(ridge_predict==test$Attrition_Flag)*100
#True Negative Rate / Specificity
Spec_ridge <- c_mat_ridge[1,1]/sum(c_mat_ridge[1,])
Spec_ridge
#Precision / Positive Predicted Value
Prec_ridge <- c_mat_ridge[2,2]/sum(c_mat_ridge[,2])
Prec_ridge
#Recall / True Positive Rate / Sensitivity
Rec_ridge <- c_mat_ridge[2,2]/sum(c_mat_ridge[2,])
Rec_ridge
#F1 Score
F1_Ridge <- 2 * (Prec_ridge * Rec_ridge)/(Prec_ridge + Rec_ridge)
F1_Ridge
#Thresholds
Threshold1 <- 0.3
Threshold2 <- 0.4
Threshold3 <- 0.5
ridge_predict_2 <- predict(ridge,test_mat,type = "response", s = opt_lambda_ridge)
pred_1 <- ifelse(ridge_predict_2 >= Threshold1 , 1,0)
pred_2 <- ifelse(ridge_predict_2 >= Threshold2 , 1,0)
pred_3 <- ifelse(ridge_predict_2 >= Threshold3 , 1,0)
#Confusion matrix
c_mat_1 <- table(test$Attrition_Flag,pred_1)
c_mat_2 <- table(test$Attrition_Flag,pred_2)
c_mat_3 <- table(test$Attrition_Flag,pred_3)
c_mat_1
c_mat_2
c_mat_3
#Accuracy
mean(pred_1==test$Attrition_Flag)*100
mean(pred_2==test$Attrition_Flag)*100
mean(pred_3==test$Attrition_Flag)*100
#True Negative Rate / Specificity
Spec_1 <- c_mat_1[1,1]/sum(c_mat_1[1,])
Spec_2 <- c_mat_2[1,1]/sum(c_mat_2[1,])
Spec_3 <- c_mat_3[1,1]/sum(c_mat_3[1,])
Spec_1
Spec_2
Spec_3
#Precision / Positive Predicted Value
Prec_1 <- c_mat_1[2,2]/sum(c_mat_1[,2])
Prec_2 <- c_mat_2[2,2]/sum(c_mat_2[,2])
Prec_3 <- c_mat_3[2,2]/sum(c_mat_3[,2])
Prec_1
Prec_2
Prec_3
#Recall / True Positive Rate / Sensitivity
Rec_1 <- c_mat_1[2,2]/sum(c_mat_1[2,])
Rec_2 <- c_mat_2[2,2]/sum(c_mat_2[2,])
Rec_3 <- c_mat_3[2,2]/sum(c_mat_3[2,])
Rec_1
Rec_2
Rec_3
#F1 Score
F1_1 <- 2 * (Prec_1 * Rec_1)/(Prec_1 + Rec_1)
F1_2 <- 2 * (Prec_2 * Rec_2)/(Prec_2 + Rec_2)
F1_3 <- 2 * (Prec_3 * Rec_3)/(Prec_3 + Rec_3)
F1_1
F1_2
F1_3
#Ridge balanced
#Creation of train_bal and test set with interactions
train_bal_int <- data.frame(train_bal)
train_bal_int$log_Total_Amt_Chng_Q4_Q1_log_Total_Trans_Amt <- train_bal$log_Total_Amt_Chng_Q4_Q1*train_bal$log_Total_Trans_Amt
train_bal_int$log_Avg_Open_To_Buy_Total_Revolving_Bal <- train_bal$log_Avg_Open_To_Buy*train_bal$Total_Revolving_Bal
train_bal_int$log_Total_Amt_Chng_Q4_Q1_Dependent_count <- train_bal$log_Total_Amt_Chng_Q4_Q1*train_bal$Dependent_count
train_bal_int$Total_Revolving_Bal_Avg_Utilization_Ratio <- train_bal$Total_Revolving_Bal*train_bal$Avg_Utilization_Ratio
test_bal_int <- data.frame(test)
test_bal_int$log_Total_Amt_Chng_Q4_Q1_log_Total_Trans_Amt <- test$log_Total_Amt_Chng_Q4_Q1*test$log_Total_Trans_Amt
test_bal_int$log_Avg_Open_To_Buy_Total_Revolving_Bal <- test$log_Avg_Open_To_Buy*test$Total_Revolving_Bal
test_bal_int$log_Total_Amt_Chng_Q4_Q1_Dependent_count <- test$log_Total_Amt_Chng_Q4_Q1*test$Dependent_count
test_bal_int$Total_Revolving_Bal_Avg_Utilization_Ratio <- test$Total_Revolving_Bal*test$Avg_Utilization_Ratio
#We transorm them into matrix withou Attrition_Flag, Months_on_book and log_Credit_Limit
train_bal_mat <- data.matrix(train_bal_int[,-c(1,8,12)])
test_bal_mat <- data.matrix(test_bal_int[,-c(1,8,12)])
ridge_bal <- cv.glmnet(train_bal_mat,train_bal$Attrition_Flag, alpha = 0, family = "binomial", type.measure = "class")
plot(ridge_bal)
opt_lambda_ridge_bal <- ridge_bal$lambda.min
opt_lambda_ridge_bal
ridge_bal_predict <- predict(ridge_bal,test_bal_mat,type = "class", s = opt_lambda_ridge_bal)
#Confusion matrix
c_mat_ridge_bal <- table(test$Attrition_Flag,ridge_bal_predict)
c_mat_ridge_bal
#Accuracy
mean(ridge_bal_predict==test$Attrition_Flag)*100
#True Negative Rate / Specificity
Spec_ridge_bal <- c_mat_ridge_bal[1,1]/sum(c_mat_ridge_bal[1,])
Spec_ridge_bal
#Precision / Positive Predicted Value
Prec_ridge_bal <- c_mat_ridge_bal[2,2]/sum(c_mat_ridge_bal[,2])
Prec_ridge_bal
#Recall / True Positive Rate / Sensitivity
Rec_ridge_bal <- c_mat_ridge_bal[2,2]/sum(c_mat_ridge_bal[2,])
Rec_ridge_bal
#F1 Score
F1_Ridge_bal <- 2 * (Prec_ridge_bal * Rec_ridge_bal)/(Prec_ridge_bal + Rec_ridge_bal)
F1_Ridge_bal
#Thresholds
Threshold1 <- 0.6
Threshold2 <- 0.7
Threshold3 <- 0.8
ridge_bal_predict_2 <- predict(ridge_bal,test_bal_mat,type = "response", s = opt_lambda_ridge_bal)
pred_1 <- ifelse(ridge_bal_predict_2 >= Threshold1 , 1,0)
pred_2 <- ifelse(ridge_bal_predict_2 >= Threshold2 , 1,0)
pred_3 <- ifelse(ridge_bal_predict_2 >= Threshold3 , 1,0)
#Confusion matrix
c_mat_1 <- table(test$Attrition_Flag,pred_1)
c_mat_2 <- table(test$Attrition_Flag,pred_2)
c_mat_3 <- table(test$Attrition_Flag,pred_3)
c_mat_1
c_mat_2
c_mat_3
#Accuracy
mean(pred_1==test$Attrition_Flag)*100
mean(pred_2==test$Attrition_Flag)*100
mean(pred_3==test$Attrition_Flag)*100
#True Negative Rate / Specificity
Spec_1 <- c_mat_1[1,1]/sum(c_mat_1[1,])
Spec_2 <- c_mat_2[1,1]/sum(c_mat_2[1,])
Spec_3 <- c_mat_3[1,1]/sum(c_mat_3[1,])
Spec_1
Spec_2
Spec_3
#Precision / Positive Predicted Value
Prec_1 <- c_mat_1[2,2]/sum(c_mat_1[,2])
Prec_2 <- c_mat_2[2,2]/sum(c_mat_2[,2])
Prec_3 <- c_mat_3[2,2]/sum(c_mat_3[,2])
Prec_1
Prec_2
Prec_3
#Recall / True Positive Rate / Sensitivity
Rec_1 <- c_mat_1[2,2]/sum(c_mat_1[2,])
Rec_2 <- c_mat_2[2,2]/sum(c_mat_2[2,])
Rec_3 <- c_mat_3[2,2]/sum(c_mat_3[2,])
Rec_1
Rec_2
Rec_3
#F1 Score
F1_1 <- 2 * (Prec_1 * Rec_1)/(Prec_1 + Rec_1)
F1_2 <- 2 * (Prec_2 * Rec_2)/(Prec_2 + Rec_2)
F1_3 <- 2 * (Prec_3 * Rec_3)/(Prec_3 + Rec_3)
F1_1
F1_2
F1_3
roc_ridge <- roc(test$Attrition_Flag ~ as.numeric(ridge_predict_2))
roc_ridge_bal <- roc(test$Attrition_Flag ~ as.numeric(ridge_bal_predict_2))
AUC_ridge <- auc(roc_ridge)
AUC_ridge_bal <- auc(roc_ridge_bal)
plot(roc_ridge, col = "black",print.auc = TRUE, auc.polygon = TRUE, max.auc.polygon = TRUE, lwd=2,print.auc.x = 0.5,print.auc.y = 0.5)
plot(roc_ridge_bal,add = TRUE,col = "blue", print.auc = TRUE, lwd=2, print.auc.x = 0.5,print.auc.y = 0.43)
#Best thresholds and Best Sensitivity and Specificity
Best_Treshold_ridge <- coords(roc_ridge,"best",best.method = "closest.topleft")$threshold
Best_pred_ridge <- ifelse(as.numeric(ridge_predict_2) >= Best_Treshold_ridge , 1,0)
Best_c_mat <- table(test$Attrition_Flag,Best_pred_ridge)
Best_Spec_ridge <- Best_c_mat[1,1]/sum(Best_c_mat[1,])
Best_Sens_ridge <- Best_c_mat[2,2]/sum(Best_c_mat[2,])
Best_Treshold_ridge_bal <- coords(roc_ridge_bal,"best",best.method = "closest.topleft")$threshold
Best_pred_ridge_bal <- ifelse(as.numeric(ridge_bal_predict_2) >= Best_Treshold_ridge_bal , 1,0)
Best_c_mat <- table(test$Attrition_Flag,Best_pred_ridge_bal)
Best_Spec_ridge_bal <- Best_c_mat[1,1]/sum(Best_c_mat[1,])
Best_Sens_ridge_bal <- Best_c_mat[2,2]/sum(Best_c_mat[2,])
#Table to showing them
Table_mat <-  matrix(c(Best_Treshold_ridge,Best_Spec_ridge,Best_Sens_ridge,Best_Treshold_ridge_bal,Best_Spec_ridge_bal,Best_Sens_ridge_bal), ncol=3, byrow=TRUE)
colnames(Table_mat) <- c("Threshold","Specificity","Sensitivity")
rownames(Table_mat) <- c("Unbalanced","Balanced")
Tab <- as.table(Table_mat)
show(Tab)
#Lasso
#Creation of train and test set with interactions
train_int <- data.frame(train)
train_int$log_Total_Amt_Chng_Q4_Q1_Total_Trans_Ct <- train$log_Total_Amt_Chng_Q4_Q1*train$Total_Trans_Ct
train_int$log_Total_Amt_Chng_Q4_Q1_log_Total_Trans_Amt <- train$log_Total_Amt_Chng_Q4_Q1*train$log_Total_Trans_Amt
train_int$log_Credit_Limit_Total_Revolving_Bal <- train$log_Credit_Limit*train$Total_Revolving_Bal
train_int$log_Total_Amt_Chng_Q4_Q1_Dependent_count <- train$log_Total_Amt_Chng_Q4_Q1*train$Dependent_count
train_int$Total_Revolving_Bal_Avg_Utilization_Ratio <- train$Total_Revolving_Bal*train$Avg_Utilization_Ratio
test_int <- data.frame(test)
test_int$log_Total_Amt_Chng_Q4_Q1_Total_Trans_Ct <- test$log_Total_Amt_Chng_Q4_Q1*test$Total_Trans_Ct
test_int$log_Total_Amt_Chng_Q4_Q1_log_Total_Trans_Amt <- test$log_Total_Amt_Chng_Q4_Q1*test$log_Total_Trans_Amt
test_int$log_Credit_Limit_Total_Revolving_Bal <- test$log_Credit_Limit*test$Total_Revolving_Bal
test_int$log_Total_Amt_Chng_Q4_Q1_Dependent_count <- test$log_Total_Amt_Chng_Q4_Q1*test$Dependent_count
test_int$Total_Revolving_Bal_Avg_Utilization_Ratio <- test$Total_Revolving_Bal*test$Avg_Utilization_Ratio
#We transorm them into matrix withou Attrition_Flag, Education_Level , Months_on_book and log_Avg_Open_To_Buy
train_mat <- data.matrix(train_int[,-c(1,5,8,14)])
test_mat <- data.matrix(test_int[,-c(1,5,8,14)])
lasso <- cv.glmnet(train_mat,train$Attrition_Flag, alpha = 1, family = "binomial", type.measure = "class")
plot(lasso)
opt_lambda_lasso <- lasso$lambda.min
opt_lambda_lasso
lasso_predict <- predict(lasso,test_mat,type = "class", s = opt_lambda_lasso)
#Confusion matrix
c_mat_lasso <- table(test$Attrition_Flag,lasso_predict)
c_mat_lasso
#Accuracy
mean(lasso_predict==test$Attrition_Flag)*100
#True Negative Rate / Specificity
Spec_lasso <- c_mat_lasso[1,1]/sum(c_mat_lasso[1,])
Spec_lasso
#Precision / Positive Predicted Value
Prec_lasso <- c_mat_lasso[2,2]/sum(c_mat_lasso[,2])
Prec_lasso
#Recall / True Positive Rate / Sensitivity
Rec_lasso <- c_mat_lasso[2,2]/sum(c_mat_lasso[2,])
Rec_lasso
#F1 Score
F1_lasso <- 2 * (Prec_lasso * Rec_lasso)/(Prec_lasso + Rec_lasso)
F1_lasso
#Thresholds
Threshold1 <- 0.3
Threshold2 <- 0.4
Threshold3 <- 0.5
lasso_predict_2 <- predict(lasso,test_mat,type = "response", s = opt_lambda_lasso)
pred_1 <- ifelse(lasso_predict_2 >= Threshold1 , 1,0)
pred_2 <- ifelse(lasso_predict_2 >= Threshold2 , 1,0)
pred_3 <- ifelse(lasso_predict_2 >= Threshold3 , 1,0)
#Confusion matrix
c_mat_1 <- table(test$Attrition_Flag,pred_1)
c_mat_2 <- table(test$Attrition_Flag,pred_2)
c_mat_3 <- table(test$Attrition_Flag,pred_3)
c_mat_1
c_mat_2
c_mat_3
#Accuracy
mean(pred_1==test$Attrition_Flag)*100
mean(pred_2==test$Attrition_Flag)*100
mean(pred_3==test$Attrition_Flag)*100
#True Negative Rate / Specificity
Spec_1 <- c_mat_1[1,1]/sum(c_mat_1[1,])
Spec_2 <- c_mat_2[1,1]/sum(c_mat_2[1,])
Spec_3 <- c_mat_3[1,1]/sum(c_mat_3[1,])
Spec_1
Spec_2
Spec_3
#Precision / Positive Predicted Value
Prec_1 <- c_mat_1[2,2]/sum(c_mat_1[,2])
Prec_2 <- c_mat_2[2,2]/sum(c_mat_2[,2])
Prec_3 <- c_mat_3[2,2]/sum(c_mat_3[,2])
Prec_1
Prec_2
Prec_3
#Recall / True Positive Rate / Sensitivity
Rec_1 <- c_mat_1[2,2]/sum(c_mat_1[2,])
Rec_2 <- c_mat_2[2,2]/sum(c_mat_2[2,])
Rec_3 <- c_mat_3[2,2]/sum(c_mat_3[2,])
Rec_1
Rec_2
Rec_3
#F1 Score
F1_1 <- 2 * (Prec_1 * Rec_1)/(Prec_1 + Rec_1)
F1_2 <- 2 * (Prec_2 * Rec_2)/(Prec_2 + Rec_2)
F1_3 <- 2 * (Prec_3 * Rec_3)/(Prec_3 + Rec_3)
F1_1
F1_2
F1_3
#Lasso Balanced
#Creation of train_bal and test set with interactions
train_bal_int <- data.frame(train_bal)
train_bal_int$log_Total_Amt_Chng_Q4_Q1_log_Total_Trans_Amt <- train_bal$log_Total_Amt_Chng_Q4_Q1*train_bal$log_Total_Trans_Amt
train_bal_int$log_Avg_Open_To_Buy_Total_Revolving_Bal <- train_bal$log_Avg_Open_To_Buy*train_bal$Total_Revolving_Bal
train_bal_int$log_Total_Amt_Chng_Q4_Q1_Dependent_count <- train_bal$log_Total_Amt_Chng_Q4_Q1*train_bal$Dependent_count
train_bal_int$Total_Revolving_Bal_Avg_Utilization_Ratio <- train_bal$Total_Revolving_Bal*train_bal$Avg_Utilization_Ratio
test_bal_int <- data.frame(test)
test_bal_int$log_Total_Amt_Chng_Q4_Q1_log_Total_Trans_Amt <- test$log_Total_Amt_Chng_Q4_Q1*test$log_Total_Trans_Amt
test_bal_int$log_Avg_Open_To_Buy_Total_Revolving_Bal <- test$log_Avg_Open_To_Buy*test$Total_Revolving_Bal
test_bal_int$log_Total_Amt_Chng_Q4_Q1_Dependent_count <- test$log_Total_Amt_Chng_Q4_Q1*test$Dependent_count
test_bal_int$Total_Revolving_Bal_Avg_Utilization_Ratio <- test$Total_Revolving_Bal*test$Avg_Utilization_Ratio
#We transorm them into matrix withou Attrition_Flag, Months_on_book and log_Credit_Limit
train_bal_mat <- data.matrix(train_bal_int[,-c(1,8,12)])
test_bal_mat <- data.matrix(test_bal_int[,-c(1,8,12)])
lasso_bal <- cv.glmnet(train_bal_mat,train_bal$Attrition_Flag, alpha = 1, family = "binomial", type.measure = "class")
plot(lasso_bal)
opt_lambda_lasso_bal <- lasso_bal$lambda.min
opt_lambda_lasso_bal
lasso_bal_predict <- predict(lasso_bal,test_bal_mat,type = "class", s = opt_lambda_lasso_bal)
#Confusion matrix
c_mat_lasso_bal <- table(test$Attrition_Flag,lasso_bal_predict)
c_mat_lasso_bal
#Accuracy
mean(lasso_bal_predict==test$Attrition_Flag)*100
#True Negative Rate / Specificity
Spec_lasso_bal <- c_mat_lasso_bal[1,1]/sum(c_mat_lasso_bal[1,])
Spec_lasso_bal
#Precision / Positive Predicted Value
Prec_lasso_bal <- c_mat_lasso_bal[2,2]/sum(c_mat_lasso_bal[,2])
Prec_lasso_bal
#Recall / True Positive Rate / Sensitivity
Rec_lasso_bal <- c_mat_lasso_bal[2,2]/sum(c_mat_lasso_bal[2,])
Rec_lasso_bal
#F1 Score
F1_lasso_bal <- 2 * (Prec_lasso_bal * Rec_lasso_bal)/(Prec_lasso_bal + Rec_lasso_bal)
F1_lasso_bal
#Thresholds
Threshold1 <- 0.6
Threshold2 <- 0.7
Threshold3 <- 0.8
lasso_bal_predict_2 <- predict(lasso_bal,test_bal_mat,type = "response", s = opt_lambda_lasso_bal)
pred_1 <- ifelse(lasso_bal_predict_2 >= Threshold1 , 1,0)
pred_2 <- ifelse(lasso_bal_predict_2 >= Threshold2 , 1,0)
pred_3 <- ifelse(lasso_bal_predict_2 >= Threshold3 , 1,0)
#Confusion matrix
c_mat_1 <- table(test$Attrition_Flag,pred_1)
c_mat_2 <- table(test$Attrition_Flag,pred_2)
c_mat_3 <- table(test$Attrition_Flag,pred_3)
c_mat_1
c_mat_2
c_mat_3
#Accuracy
mean(pred_1==test$Attrition_Flag)*100
mean(pred_2==test$Attrition_Flag)*100
mean(pred_3==test$Attrition_Flag)*100
#True Negative Rate / Specificity
Spec_1 <- c_mat_1[1,1]/sum(c_mat_1[1,])
Spec_2 <- c_mat_2[1,1]/sum(c_mat_2[1,])
Spec_3 <- c_mat_3[1,1]/sum(c_mat_3[1,])
Spec_1
Spec_2
Spec_3
#Precision / Positive Predicted Value
Prec_1 <- c_mat_1[2,2]/sum(c_mat_1[,2])
Prec_2 <- c_mat_2[2,2]/sum(c_mat_2[,2])
Prec_3 <- c_mat_3[2,2]/sum(c_mat_3[,2])
Prec_1
Prec_2
Prec_3
#Recall / True Positive Rate / Sensitivity
Rec_1 <- c_mat_1[2,2]/sum(c_mat_1[2,])
Rec_2 <- c_mat_2[2,2]/sum(c_mat_2[2,])
Rec_3 <- c_mat_3[2,2]/sum(c_mat_3[2,])
Rec_1
Rec_2
Rec_3
#F1 Score
F1_1 <- 2 * (Prec_1 * Rec_1)/(Prec_1 + Rec_1)
F1_2 <- 2 * (Prec_2 * Rec_2)/(Prec_2 + Rec_2)
F1_3 <- 2 * (Prec_3 * Rec_3)/(Prec_3 + Rec_3)
F1_1
F1_2
F1_3
roc_lasso <- roc(test$Attrition_Flag ~ as.numeric(lasso_predict_2))
roc_lasso_bal <- roc(test$Attrition_Flag ~ as.numeric(lasso_bal_predict_2))
AUC_lasso <- auc(roc_lasso)
AUC_lasso_bal <- auc(roc_lasso_bal)
plot(roc_lasso, col = "black",print.auc = TRUE, auc.polygon = TRUE, max.auc.polygon = TRUE, lwd=2,print.auc.x = 0.5,print.auc.y = 0.5)
plot(roc_lasso_bal,add = TRUE,col = "blue", print.auc = TRUE, lwd=2, print.auc.x = 0.5,print.auc.y = 0.43)
#Best thresholds and Best Sensitivity and Specificity
Best_Treshold_lasso <- coords(roc_lasso,"best",best.method = "closest.topleft")$threshold
Best_pred_lasso <- ifelse(as.numeric(lasso_predict_2) >= Best_Treshold_ridge , 1,0)
Best_c_mat <- table(test$Attrition_Flag,Best_pred_lasso)
Best_Spec_lasso <- Best_c_mat[1,1]/sum(Best_c_mat[1,])
Best_Sens_lasso <- Best_c_mat[2,2]/sum(Best_c_mat[2,])
Best_Treshold_lasso_bal <- coords(roc_lasso_bal,"best",best.method = "closest.topleft")$threshold
Best_pred_lasso_bal <- ifelse(as.numeric(lasso_bal_predict_2) >= Best_Treshold_lasso_bal , 1,0)
Best_c_mat <- table(test$Attrition_Flag,Best_pred_lasso_bal)
Best_Spec_lasso_bal <- Best_c_mat[1,1]/sum(Best_c_mat[1,])
Best_Sens_lasso_bal <- Best_c_mat[2,2]/sum(Best_c_mat[2,])
#Table to showing them
Table_mat <-  matrix(c(Best_Treshold_lasso,Best_Spec_lasso,Best_Sens_lasso,Best_Treshold_lasso_bal,Best_Spec_lasso_bal,Best_Sens_lasso_bal), ncol=3, byrow=TRUE)
colnames(Table_mat) <- c("Threshold","Specificity","Sensitivity")
rownames(Table_mat) <- c("Unbalanced","Balanced")
Tab <- as.table(Table_mat)
show(Tab)
